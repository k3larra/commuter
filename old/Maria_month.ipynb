{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Marias](Maria.md) prediction after one month use\n",
    "ID: tnK534JMwwfhvUEycn69HPbhqkt2\n",
    "Marias data is presented below after one week of use. The predictions reflects her persona and travel patterns, and explores preditions using Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0 Python version: 2.7.12 (default, Dec  4 2017, 14:50:18) \n",
      "[GCC 5.4.0 20160609]\n",
      "/home/lars_rauer/notebookcommute/notebooks/commuter/pendlaren/tnK534JMwwfhvUEycn69HPbhqkt2\n",
      "Retraining the model from scratch\n",
      "Prediction is \"8121680140\" (100.0%), expected \"8121680000\"\n",
      "Prediction is \"8121680140\" (99.4%), expected \"8121680000\"\n",
      "Prediction is \"8121680140\" (99.2%), expected \"8121680000\"\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import sys, getopt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Tensorflow version: \"+tf.VERSION+ \" Python version: \"+sys.version)\n",
    "TABLE = 'tnK534JMwwfhvUEycn69HPbhqkt2'\n",
    "ROOT_DIR = \"pendlaren\"\n",
    "export_dir = os.path.join(ROOT_DIR, TABLE)\n",
    "print(os.path.abspath(export_dir))\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "def load_data(to_row,y_name='journey'):\n",
    "    train = pd.read_csv(\"data/tnK534JMwwfhvUEycn69HPbhqkt2.csv\",comment=\"-\",skiprows=range(to_row,6762))\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "    #I know WRONG\n",
    "    test =pd.read_csv(\"data/tnK534JMwwfhvUEycn69HPbhqkt2.csv\",comment=\"-\",skiprows=range(1,6000))\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "    train_y.astype(str)\n",
    "    return (train_x, train_y.astype(str)), (test_x, test_y.astype(str))\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "#print(my_feature_columns)\n",
    "#Below LinearClassifier (Logistic regression) https://www.tensorflow.org/get_started/feature_columns\n",
    "detectedActivity = tf.feature_column.categorical_column_with_identity(key=\"detectedActivity\",num_buckets=7,default_value=0)\n",
    "geoHash = tf.feature_column.categorical_column_with_hash_bucket(\"geoHash\",100,dtype=tf.int64)\n",
    "minuteOfDay = tf.feature_column.numeric_column(\"minuteOfDay\")\n",
    "weekDay = tf.feature_column.categorical_column_with_identity(key=\"weekday\",num_buckets=7,default_value=0)\n",
    "feature_columns = [detectedActivity,geoHash,minuteOfDay,weekDay]\n",
    "\n",
    "retrainAll = True\n",
    "if retrainAll:\n",
    "    try:\n",
    "        shutil.rmtree(export_dir)\n",
    "        print(\"Retraining the model from scratch\")\n",
    "    except:\n",
    "        print(\"Incremental training\")\n",
    "        \n",
    "(train_x, train_y), (test_x, test_y)= load_data(to_row=500)\n",
    "label_vocabulary = np.unique(train_y).tolist()\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n",
    "                                            n_classes=len(label_vocabulary),\n",
    "                                            model_dir=export_dir,\n",
    "                                            label_vocabulary=label_vocabulary)\n",
    "\n",
    "classifier.train(input_fn=lambda:train_input_fn(train_x, train_y, 100), max_steps=100)\n",
    "\n",
    "expected = ['8121680000', '8121680000', '8121680000']\n",
    "predict_x = {\n",
    "    'detectedActivity': [7,7,7],\n",
    "    'geoHash': [1242479403, 124247900, 12424800],\n",
    "    'minuteOfDay': [531,480,600],\n",
    "    'weekday': [2,1,5],\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:eval_input_fn(predict_x,labels=None, batch_size=10))\n",
    "\n",
    "template = ('Prediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "    print(template.format(label_vocabulary[class_id],100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
