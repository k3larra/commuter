{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate commuter data over time\n",
    "The code below takes labelled data, trains a NN using a growing amount of training data.\n",
    "Since this is done itaratively and compared to predictions towards a test set an estimation can be made over how fast the model converges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from fastai import *          # Quick accesss to most common functionality\n",
    "from fastai.tabular import *  # Quick accesss to tabular functionality     # Access to example data provided with fastai\n",
    "from fastai.vision import *\n",
    "PATH=\"../data/\"\n",
    "dep_var = 'journey'\n",
    "cat_names = [\"detectedActivity\",\"weekday\"]\n",
    "cont_names =[\"geoHash\",\"minuteOfDay\"]\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "# Uncomment active user\n",
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "\n",
    "def predict_journeys(learner,dataset):\n",
    "    \"This can be somthinthing that is already in the framework... Confusion matrix adding test set..., but I have some hard time finding the data in the learner\"\n",
    "    \"If filename exists all wrong predictions will be saved.\"\n",
    "    result = 0\n",
    "    accuracy = 0\n",
    "    #details = []\n",
    "    for x in range(0,dataset.shape[0]):\n",
    "        correct = dataset.iloc[x].journey  #remove journey\n",
    "        predicted = learner.predict(dataset.iloc[x]);\n",
    "        #print(str(correct)+\":\"+str(predicted[0]))\n",
    "        if (str(correct)==str(predicted[0])):\n",
    "            result=result+1\n",
    "        #details.append([str(correct),str(predicted[0]),str(round(predicted[2].max().item()))])\n",
    "    accuracy=result/dataset.shape[0]\n",
    "    return(accuracy)\n",
    "    \n",
    "def predict_journey(learner,detectedActivity,geoHash,minuteOfday,weekday):\n",
    "    data = np.array([['','detectedActivity','geoHash','minuteOfDay','weekday'],\n",
    "                [\"row1\",detectedActivity,geoHash,minuteOfday,weekday]])            \n",
    "    dr=pd.DataFrame(data=data[1:,1:],\n",
    "                    index=data[1:,0],\n",
    "                    columns=data[0,1:]).astype(np.int64)\n",
    "    predicted = learner.predict(dr.iloc[0])\n",
    "    return(predicted[0],str(round(predicted[2].max().item(),2)))\n",
    "\n",
    "# def evaluate_learner(rows_per_training:int,trainingset,testset,length_teachingSet=0): #Minimal start for any training can bne used for offset\n",
    "#     \"Asumes a constant amount of rows per day in training\"\n",
    "#     result = []\n",
    "#     if length_teachingSet>0: #do teaching and add thus add startdata for day 0\n",
    "#         start_point = length_teachingSet\n",
    "#     else: #no teachin set exists add row for day 0\n",
    "#         result.append([0,0,0])\n",
    "#         start_point = rows_per_training\n",
    "#     for rows in range(start_point,len(trainingset),rows_per_training):\n",
    "#         df=trainingset[0:rows]\n",
    "#         df=make_shure_we_got_enough_rows(df)\n",
    "#         #valid_idx= list(np.random.randint(0,len(df),int(len(df)*0.1))) #If we want 10% random instead of same 10 rows...\n",
    "#         valid_idx = list(range(int(len(df)*0.9), len(df)))  #Here always last 10% is used for validation\n",
    "#         data = (TabularList.from_df(df, path=\"models\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "#                 .split_by_idx(valid_idx)\n",
    "#                 .label_from_df(cols=dep_var)\n",
    "#                 .databunch())\n",
    "#         learner=tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "#         learner.fit_one_cycle(30)\n",
    "#         #learner.fit(5, 1e-2)\n",
    "#         sizeR = len(learner.recorder.metrics)\n",
    "#         resultTest = predict_journeys(learner,testset)\n",
    "#         result.append([rows,learner.recorder.metrics[sizeR-1][0].item(),resultTest])\n",
    "#     return (result)\n",
    "\n",
    "def evaluate_learner_varying_batch_size(trainingset,testset,teachingset=None):\n",
    "    \"Trains in batches. A new batch starts with -- and ends with next row that starts with --\"\n",
    "    \"Training is done from the first row that starts with -- until next row that starts with -- and thus training is done with a growing amount of rows\"\n",
    "    \"Rows in a batch are duplicated so there are aways at least 500 rows for training (no new information added and no other data agumentation)\"\n",
    "    \"If an initial teaching set is used add the rows first without an initial row starting with --.\"\n",
    "    result = []\n",
    "    iterativeTrainingSet = DataFrame(testset[0:0]) #copy structure\n",
    "    for row in trainingset.itertuples():\n",
    "        if row[1].startswith('--'): #New batch\n",
    "            if len(iterativeTrainingSet)>0 or teachingset is not None:\n",
    "                df=iterativeTrainingSet.copy() #crete another! copy to train from\n",
    "                df = df.astype({'detectedActivity':'int','geoHash':'int', 'minuteOfDay':'int','weekday':'int',\"journey\":'int'})\n",
    "                if teachingset is not None:\n",
    "                    df = pd.concat([teachingset,df])\n",
    "                df=make_shure_we_got_enough_rows(df)  ##Add rows so it is big enough to train from\n",
    "                valid_idx= list(np.random.randint(0,len(df),int(len(df)*0.1))) #If we want 10% random instead of same 10 rows...\n",
    "                #valid_idx = list(range(int(len(df)*0.9), len(df)))  #Here always last 10% is used for validation\n",
    "                data = (TabularList.from_df(df, path=\"models\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                    .split_by_idx(valid_idx)\n",
    "                    .label_from_df(cols=dep_var)\n",
    "                    .databunch())\n",
    "                learner=tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "                learner.fit_one_cycle(20)\n",
    "                sizeR = len(learner.recorder.metrics)\n",
    "                resultTest = predict_journeys(learner,testset)\n",
    "                result.append([len(iterativeTrainingSet),learner.recorder.metrics[sizeR-1][0].item(),resultTest])\n",
    "            else: ##it is first row or no teaching data exists(Cold start) so no training is possible\n",
    "                result.append([0,0,0])\n",
    "        else: # just add the for to training set\n",
    "            iterativeTrainingSet = iterativeTrainingSet.append(trainingset.iloc[row[0]])\n",
    "    return (result)\n",
    "\n",
    "def make_shure_we_got_enough_rows(dataset,minrows=1000):\n",
    "    \"If the dataset has fewer rows that minrows, whole dataset copies will be added at the end until at least minrows exists\"\n",
    "    newset = pd.DataFrame()\n",
    "    while newset.shape[0]<minrows:\n",
    "        newset = pd.concat([newset,dataset])\n",
    "    return(newset)\n",
    "\n",
    "def save_results(filename,result):\n",
    "    a = np.asarray(result)\n",
    "    np.savetxt(\"saved/\"+filename,a,delimiter=',',fmt=\"%10.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learner_daily(trainingset,testset,teachingset=None):\n",
    "    \"Trains in batches. A new batch starts with -- and ends with next row that starts with --\"\n",
    "    \"Training is done from the first row that starts with -- until next row that starts with -- and thus training is done with a growing amount of rows\"\n",
    "    \"Rows in a batch are duplicated so there are aways at least 500 rows for training (no new information added and no other data agumentation)\"\n",
    "    \"If an initial teaching set is used add the rows first without an initial row starting with --.\"\n",
    "    result = []\n",
    "    oldDayNumber=-1\n",
    "    iterativeTrainingSet = DataFrame(testset[0:0]) #copy structure\n",
    "    #for index, row in trainingset.iterrows():\n",
    "    for row in trainingset.itertuples():\n",
    "        if row[0]+1==len(trainingset):\n",
    "            print(\"lastRow REALLY\")\n",
    "        if oldDayNumber != row[4] or row[0]+1==len(trainingset): #A new day!\n",
    "            oldDayNumber = row[4]\n",
    "            if (row[0]+1==len(trainingset)):   ##if last line add it for training\n",
    "                iterativeTrainingSet = iterativeTrainingSet.append(trainingset.iloc[row[0]])\n",
    "            if len(iterativeTrainingSet)>0 or teachingset is not None:\n",
    "                df=iterativeTrainingSet.copy() #crete another! copy to train from\n",
    "                df = df.astype({'detectedActivity':'int','geoHash':'int', 'minuteOfDay':'int','weekday':'int',\"journey\":'int'})\n",
    "                if teachingset is not None:\n",
    "                    df = pd.concat([teachingset,df])\n",
    "                df=make_shure_we_got_enough_rows(df)  ##Add rows so it is big enough to train from\n",
    "                valid_idx= list(np.random.randint(0,len(df),int(len(df)*0.1))) #If we want 10% random instead of same 10% rows...\n",
    "                #valid_idx = list(range(int(len(df)*0.9), len(df)))  #Here always last 10% is used for validation\n",
    "                data = (TabularList.from_df(df, path=\"models\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                    .split_by_idx(valid_idx)\n",
    "                    .label_from_df(cols=dep_var)\n",
    "                    .databunch())\n",
    "                learner=tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "                learner.fit_one_cycle(20)\n",
    "                resultTest = predict_journeys(learner,testset)\n",
    "                result.append([len(iterativeTrainingSet),learner.recorder.metrics[len(learner.recorder.metrics)-1][0].item(),resultTest])\n",
    "                iterativeTrainingSet = iterativeTrainingSet.append(trainingset.iloc[row[0]])\n",
    "            else: ##it is first row or no teaching data exists(Cold start) so no training is possible\n",
    "                result.append([0,0,0])\n",
    "                iterativeTrainingSet = iterativeTrainingSet.append(trainingset.iloc[row[0]])\n",
    "        else: # same old day add the row  \n",
    "            iterativeTrainingSet = iterativeTrainingSet.append(trainingset.iloc[row[0]])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative training daily during the initial 14 days of use.\n",
    "We train every day from the start; the data is created using the personas travel patterns over the first 14 days of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trainingset = pd.read_csv(PATH+user+\"_start14days.csv\")\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "resultMLTrain=evaluate_learner_varying_batch_size(trainingset,testset)\n",
    "save_results(\"Iterative_time_\"+user+\".csv\",resultMLTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trainingset = pd.read_csv(PATH+user+\"_start14days2.csv\")\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "resultMLTrain=evaluate_learner_daily(trainingset,testset)\n",
    "#save_results(\"Iterative_time_\"+user+\".csv\",resultMLTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [6, 1.0, 0.05291005291005291], [14, 1.0, 0.005291005291005291], [20, 1.0, 0.873015873015873], [27, 1.0, 0.9047619047619048], [33, 1.0, 0.8994708994708994], [43, 1.0, 0.9047619047619048], [52, 1.0, 0.9074074074074074], [59, 0.9599999785423279, 0.9021164021164021], [68, 0.9509803652763367, 0.9074074074074074], [77, 0.9900000095367432, 0.9047619047619048], [85, 0.9803921580314636, 0.9047619047619048], [93, 0.970588207244873, 0.91005291005291], [103, 0.9805825352668762, 0.9047619047619048]]\n"
     ]
    }
   ],
   "source": [
    "print(resultMLTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [5, 1.0, 0.023809523809523808], [12, 1.0, 0.018518518518518517], [17, 1.0, 0.9021164021164021], [23, 1.0, 0.9047619047619048], [28, 1.0, 0.9047619047619048], [37, 1.0, 0.8994708994708994], [45, 1.0, 0.9047619047619048], [51, 0.9803921580314636, 0.8915343915343915], [59, 0.9900000095367432, 0.9153439153439153], [67, 1.0, 0.91005291005291], [74, 0.9902912378311157, 0.9047619047619048], [81, 0.9904761910438538, 0.8941798941798942]]\n"
     ]
    }
   ],
   "source": [
    "print(resultMLTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative training daily during the initial 14 days of use, a teaching set is added to the training data.\n",
    "Here we add an idealised teaching set in the beginning of our dataset. Then we start training from day 0 using only the data in the teachingset and contiuing as in the section above.\n",
    "Teachingset:\n",
    "\n",
    "Andrea:\n",
    "[Verbose](https://github.com/k3larra/commuter/blob/master/data/ehaBtfOPDNZjzy1MEvjQmGo4Zv12_teaching_set.csv)\n",
    "[Minimal](../commuter/data/ehaBtfOPDNZjzy1MEvjQmGo4Zv12_teaching_set_minimal.csv)\n",
    "\n",
    "Björn:\n",
    "[Verbose](https://github.com/k3larra/commuter/blob/master/data/hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2_teaching_set.csv)\n",
    "[Minimal](../commuter/data/hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2_teaching_set_minimal.csv)\n",
    "\n",
    "Maria:\n",
    "[Verbose](https://github.com/k3larra/commuter/blob/master/data/tnK534JMwwfhvUEycn69HPbhqkt2_teaching_set.csv)\n",
    "[Minimal](../commuter/data/tnK534JMwwfhvUEycn69HPbhqkt2_teaching_set_minimal.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#teachingset = pd.read_csv(PATH+user+\"_teaching_set.csv\")\n",
    "teachingset = pd.read_csv(PATH+user+\"_teaching_set_minimal.csv\")\n",
    "trainingset = pd.read_csv(PATH+user+\"_start14days.csv\")\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "resultMLTrain=evaluate_learner_varying_batch_size(trainingset,testset,teachingset)\n",
    "#save_results(\"Iterative_time_teach\"+user+\".csv\",resultMLTrain)\n",
    "save_results(\"Iterative_time_teach_minimal\"+user+\".csv\",resultMLTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative training daily during the initial 14 days of use, we use a model traied on data from the same distribution as the test set.\n",
    "As a comparison to the training above this model is trained using data created using the persona envisioning one year of use. The trained model is pretrained and deployed, the model is updated iteratively using incoming data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "teachingset = pd.read_csv(PATH+user+\"_train_valid.csv\")\n",
    "trainingset = pd.read_csv(PATH+user+\"_start14days.csv\")\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "resultMLTrain=evaluate_learner_varying_batch_size(trainingset,testset,teachingset)\n",
    "save_results(\"Iterative_time_supervised\"+user+\".csv\",resultMLTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results\n",
    "Prints results from training done in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "import pandas as pd\n",
    "personas = pd.read_csv(\"saved/personas.csv\")\n",
    "#Uncomment to look at individual result for other users that the default declared above.\n",
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "#user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "personaname = personas.loc[personas['id']==user]\n",
    "learning_ax0= pd.read_csv(\"saved/Iterative_time_\"+user+\".csv\",header=None).values\n",
    "teaching_ax0 = pd.read_csv(\"saved/Iterative_time_teach\"+user+\".csv\",header=None).values\n",
    "teaching_min_ax0 = pd.read_csv(\"saved/Iterative_time_teach_minimal\"+user+\".csv\",header=None).values\n",
    "supervised_min_ax0 = pd.read_csv(\"saved/Iterative_time_supervised\"+user+\".csv\",header=None).values\n",
    "fig, ax0 = plt.subplots()\n",
    "ax0.set_title(personaname ['name'].iloc[0]+\":\"+user)\n",
    "ax0.plot(learning_ax0[:,2],label='Iterative learning')\n",
    "ax0.plot(teaching_ax0[:,2],label='Teaching')\n",
    "ax0.plot(teaching_min_ax0[:,2],label='Teaching using minimal data')\n",
    "ax0.plot(supervised_min_ax0[:,2],label='Pretrained model')\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Accuracy')\n",
    "legend = ax0.legend(loc='lower right', shadow=True, fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "Used explorative code can be removed later. Ignore or read adds nothing really except expainability on my level of thinking.\n",
    "## Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Training without teaching set on shuffeled training data\n",
    "trainingset = pd.read_csv(PATH+user+\"_train_valid.csv\")\n",
    "#Take 14 days\n",
    "trainingset=trainingset[0:int(trainingset.shape[0]/24)]\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "size=trainingset.shape[0]\n",
    "rows_per_training = int(size/14) # will result in about one training per day\n",
    "print(rows_per_training)\n",
    "print(len(trainingset))\n",
    "resultTrain = evaluate_learner(rows_per_training,trainingset,testset)\n",
    "save_results(\"Iterative_\"+user+\".csv\",resultTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Training with teaching set on shuffeled training data\n",
    "trainingset = pd.read_csv(\"data/\"+user+\"_train_valid.csv\")\n",
    "#Take first two weeks\n",
    "trainingset=trainingset[0:int(trainingset.shape[0]/24)]\n",
    "size=trainingset.shape[0]\n",
    "rows_per_training = int(size/14) # will result in about one training per day\n",
    "teachingset = pd.read_csv(PATH+user+\"_teaching_set.csv\")\n",
    "trainingset = pd.concat([teachingset,trainingset])\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "resultTeach = evaluate_learner(rows_per_training,trainingset,testset,len(teachingset))\n",
    "save_results(\"TeachingIterative_\"+user+\".csv\",resultTeach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching\n",
    "If an idealised teaching set is created that contains all combinations of categorical columns (activity and weekday) combined with a time placed in the middle of the timespan for the scenario and location at the departure station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "#user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "filename = user+\"_teaching_set_minimal.csv\"\n",
    "teachingSet = pd.read_csv(PATH+filename)\n",
    "teachingSet=make_shure_we_got_enough_rows(teachingSet)\n",
    "valid_idx= list(np.random.randint(0,len(teachingSet),int(len(teachingSet)*0.1))) #If we want 10% random instead of same 10 rows...\n",
    "#valid_idx = list(range(int(len(teachingSet)*0.9), len(teachingSet)))  #Here always last 10% is used for validation\n",
    "data = (TabularList.from_df(teachingSet, path=user, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "    .split_by_idx(valid_idx)\n",
    "    .label_from_df(cols=dep_var)\n",
    "    .databunch())\n",
    "learner=tabular_learner(data, layers=[200,100], metrics=accuracy,callback_fns=ShowGraph)\n",
    "learner.fit_one_cycle(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test one cycle with part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "#user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "filename = user+\"_train_valid.csv\"\n",
    "dataSetIn = pd.read_csv(PATH+filename)\n",
    "dataSet=dataSetIn[0:3000]\n",
    "dataSet=make_shure_we_got_enough_rows(dataSet)\n",
    "#valid_idx = list(np.random.randint(0,len(dataSet),int(len(dataSet)*0.1)))\n",
    "valid_idx = list(range(int(len(dataSet)*0.9), len(dataSet)))  #Here always last 10% is used for validation \n",
    "data = (TabularList.from_df(dataSet, path=user, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                .split_by_idx(valid_idx)\n",
    "                .label_from_df(cols=dep_var)\n",
    "                .databunch())\n",
    "learner=tabular_learner(data, layers=[200,100], metrics=accuracy,callback_fns=ShowGraph)\n",
    "#%time learner.fit_one_cycle(10)\n",
    "learner.fit_one_cycle(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set\n",
    "Evaluate using the test set on a trained \"learner\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "#user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "filenameTest = user+\"_test.csv\"\n",
    "datasetTest = pd.read_csv(PATH+filenameTest)\n",
    "accuracyTest,details = predict_journeys(learner,datasetTest)\n",
    "print(accuracyTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracyTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "dd=np.array(details)\n",
    "for x in range (0,len(details)):\n",
    "    if dd.item((x,0))!=dd.item((x,1)):\n",
    "        print(dd[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on individual journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction,accuracy = predict_journey(learner,3,1243184839,603,6) #8107980000 \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" Andrea\n",
    "#prediction,accuracy = predict_journey(learner,3,1242202139,806,6) #8033880159 \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "prediction,accuracy = predict_journey(learner,3,1242479279,527,5) #8121680000 \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "print(\"Predicted journey from: \"+str(prediction)[0:5]+\" to \" +str(prediction)[5:10]+ \" with accuracy \"+str(accuracy));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y,losses = learner.get_preds(with_loss=True)\n",
    "interp = ClassificationInterpretation(data, preds, y, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interesting but why is detectedactivity presented? It should be the journeys????\n",
    "#Contue working on this since it gives information on wrong predictions. Real travels can be added so it can be read by a domai expert.\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for 14 days iterative training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "##read 14 days try\n",
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "#user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "filenameTrain = user+\"_start14days.csv\"\n",
    "datasetTrain = pd.read_csv(PATH+filenameTrain)\n",
    "iterativeTrainingSet = DataFrame(datasetTrain[0:0])\n",
    "testset = pd.read_csv(PATH+user+\"_test.csv\")\n",
    "result = []\n",
    "for row in datasetTrain.itertuples():\n",
    "    if row[1].startswith('--'):\n",
    "        print(\"Train until: \"+row[1]) ##New batch\n",
    "        if len(iterativeTrainingSet)>0:\n",
    "            df = iterativeTrainingSet.copy()\n",
    "            df = df.astype({'detectedActivity':'int','geoHash':'int', 'minuteOfDay':'int','weekday':'int',\"journey\":'int'})\n",
    "            #print(df.head(20))\n",
    "            df=make_shure_we_got_enough_rows(df)\n",
    "            valid_idx = list(range(int(len(df)*0.9), len(df)))  #Here always last 10% is used for validation \n",
    "            data = (TabularList.from_df(df, path=user, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                .split_by_idx(valid_idx)\n",
    "                .label_from_df(cols=dep_var)\n",
    "                .databunch())\n",
    "            learner=tabular_learner(data, layers=[200,100], metrics=accuracy,callback_fns=ShowGraph)\n",
    "            learner.fit_one_cycle(20)\n",
    "            resultTest = predict_journeys(learner,testset)\n",
    "            print(\"Prediction: \"+str(resultTest))\n",
    "            sizeR = len(learner.recorder.metrics)\n",
    "            result.append([len(iterativeTrainingSet),learner.recorder.metrics[sizeR-1][0].item(),resultTest])\n",
    "        else:\n",
    "            result.append([0,0,0])\n",
    "    else:\n",
    "        iterativeTrainingSet = iterativeTrainingSet.append(datasetTrain.iloc[row[0]])\n",
    "print(result)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(name=user)\n",
    "data.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model and fnfer journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *          # Quick accesss to most common functionality\n",
    "from fastai.tabular import *  # Quick accesss to tabular functionality     # Access to example data provided with fastai\n",
    "# Uncomment active user\n",
    "#user = \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "user = \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" #Andrea\n",
    "#user = \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "def predict_journey(learner,detectedActivity,geoHash,minuteOfday,weekday):\n",
    "    data = np.array([['','detectedActivity','geoHash','minuteOfDay','weekday'],\n",
    "                [\"row1\",detectedActivity,geoHash,minuteOfday,weekday]])            \n",
    "    dr=pd.DataFrame(data=data[1:,1:],\n",
    "                    index=data[1:,0],\n",
    "                    columns=data[0,1:]).astype(np.int64)\n",
    "    predicted = learner.predict(dr.iloc[0])\n",
    "    return(predicted[0],str(round(predicted[2].max().item(),2)))\n",
    "data = TabularDataBunch.load_empty(user)\n",
    "learn_andrea = tabular_learner(data, layers=[200,100])\n",
    "learn_andrea.load(user);\n",
    "prediction,accuracy = predict_journey(learn_andrea,3,1243184839,603,6) #8107980000 \"ehaBtfOPDNZjzy1MEvjQmGo4Zv12\" Andrea\n",
    "#prediction,accuracy = predict_journey(learner,3,1242202139,806,6) #8033880159 \"hCWCulj7M1aMVyd0Fm0Eqrv8q1Q2\" #Bjorn\n",
    "#prediction,accuracy = predict_journey(learn_andrea,3,1242479279,527,5) #8121680000 \"tnK534JMwwfhvUEycn69HPbhqkt2\" #Maria\n",
    "print(\"Predicted journey from: \"+str(prediction)[0:5]+\" to \" +str(prediction)[5:10]+ \" with accuracy \"+str(accuracy));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
